{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LongTextGenerationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ll3091/ANLY-580-01-NLP-Project/blob/master/LongTextGenerationModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mrB_G9Ab1QR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Project: Text Generation Model Training"
      ]
    },
    {
      "metadata": {
        "id": "PtS7V74X1VaL",
        "colab_type": "code",
        "outputId": "b9f3f188-dc8e-438e-c65e-eea066486592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "# source https://github.com/minimaxir/textgenrnn\n",
        "! pip install textgenrnn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textgenrnn in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.8.0)\n",
            "Requirement already satisfied: keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (0.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3QC0MFkv2RJ6",
        "colab_type": "code",
        "outputId": "1098a603-0cd8-4d2e-da99-93c75464b72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from textgenrnn import textgenrnn\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-aqy7vb56W0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# connect to Google drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk5CAFIn6I3D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8yE6_sA8a2P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls gdrive/'My Drive'/NLPProject"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzD5Qs813CyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level RNNs"
      ]
    },
    {
      "metadata": {
        "id": "TvcIyOoA2Usc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 40,\n",
        "    'max_words': 300,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': False,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 10,\n",
        "    'gen_epochs': 2,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 150,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9lbt3CJ3M4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Paper Abstracts"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GpCxwNQNx1u2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/abstracts.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0ol0b6xrx1vC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2754
        },
        "outputId": "fd505b66-0d84-4e2f-b1f1-ca3f4e2a30f8"
      },
      "cell_type": "code",
      "source": [
        "# ~2.5 hours to train\n",
        "model_name = 'char_abstracts'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 1,875,637 character sequences.\n",
            "Epoch 1/10\n",
            "3663/3663 [==============================] - 888s 243ms/step - loss: 1.5016 - val_loss: 1.1075\n",
            "Epoch 2/10\n",
            "3663/3663 [==============================] - 882s 241ms/step - loss: 1.0658 - val_loss: 1.0246\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "l and convex optimization and provide a relational statistical analysis of the optimization problem is a structured method for the problem of a simple\n",
            "\n",
            "he statistical algorithm for the problem of the problem of the problem of the proposed algorithm for the statistical properties of the problem of mult\n",
            "\n",
            "for the later setting of the statistical problem in the state of the art positive and statistical and statistical results are properly improve the pro\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "raining regret as a basis of a statistical advantage of $\\epsilon)$ and the recent is are demonstrate that our approach. We consider the proposed algo\n",
            "\n",
            "reviously exploit delineage as a sets and state-of-the-arrazment of the principle of-the algorithms. The proposed algorithm is a structured local prop\n",
            "\n",
            "ze of convex optimization. As a sequence of the component random function is accompletion to exploiting a low-rank statistical properties of the onlin\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "ion process. Provably, we BAyessal?s handle optimization are does inperting they has robust prese. We validate the resource of networks based on task.\n",
            "\n",
            "tion. Our result. To approximately) previously analyze the kernel submodule oracle (2.). The new level of function with or linear adversarysiggand upd\n",
            "\n",
            "ch capture our system decomposition. We demonstrate that the over agent and imposition on next in training points, entarraming ufit. In the sparse clu\n",
            "\n",
            "Epoch 3/10\n",
            "3663/3663 [==============================] - 882s 241ms/step - loss: 0.9993 - val_loss: 0.9856\n",
            "Epoch 4/10\n",
            "3663/3663 [==============================] - 881s 241ms/step - loss: 0.9578 - val_loss: 0.9600\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            " applied to a single convex optimization problem is a statistical approach to succeed as a state-of-the-art algorithm that are able to allow the probl\n",
            "\n",
            "imental result is a constrained and state-of-the-art algorithms for a single point of the proposed method is able to a simple and analyze a state-of-t\n",
            "\n",
            "ithm that can be achieved by a single computation of the proposed method that are able to apply the state of the art algorithm that are able to a sing\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "ional costs or bounded or interpretation of such a sparse problem for a first pairwise consistency of the proposed method for model learning algorithm\n",
            "\n",
            "ate-of-the-art comparation tasks. We show that the proposed algorithm called the statistical algorithm for distributed data summarization problem and \n",
            "\n",
            "the-art approach and its simple approximate points of computational points of the most of statistical processing when the distributed regret of a gene\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "eved reduce the constraint to label representations in more accurate.  In this paper, we ochigud a quignous intracth of Block isots.\n",
            "\n",
            "Cker LoR Correla\n",
            "\n",
            "lgorithms to significantly automatically appocad to large half of highwark strongly one of the total choice are orn is under wayanography. Principle t\n",
            "\n",
            "f conditions are only captured by cope map out bounds of 1.5jexsments that based on global feature algorithms in higher order components, and empirica\n",
            "\n",
            "Epoch 5/10\n",
            "3663/3663 [==============================] - 889s 243ms/step - loss: 0.9249 - val_loss: 0.9406\n",
            "Epoch 6/10\n",
            "3663/3663 [==============================] - 895s 244ms/step - loss: 0.8955 - val_loss: 0.9258\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            " structure of the distribution of the structure of the proposed method to solve the proposed method to improve the state-of-the-art algorithms for the\n",
            "\n",
            "tic gradient descent (SGD) algorithm to provide a novel approach to solve the subject of the state of the art approaches. We develop a new method to a\n",
            "\n",
            "ngle problem in the setting of statistical points in the structure of the matrix and the statistical principal component analysis (SGD) algorithm for \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            " of the structure-unsupervised distribution over the structure of the proposed algorithm to analyze self-and-memory continuous matrices. The resulting\n",
            "\n",
            "ling and a specialization of the data of the information of the standard structure of the standard structure of the true distributions. Our study is t\n",
            "\n",
            "computational information to the proposed algorithm to the common probabilistic model. We show that the proposed algorithms for semi-supervised learni\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "by computing tatcon serious power. This feedback is the inputor using a subset of surprisingly convex, minimum - structured image learning for which t\n",
            "\n",
            "hms, which extends the attempts to an image space is agtust a nuistent --anisometry similarity interyingage in order to anomaly the recovery of earl. \n",
            "\n",
            " new approaching for CP solutions for the state-of-the-art encoder neural networks with empirical hierarchical (those), is amenable to communication, \n",
            "\n",
            "Epoch 7/10\n",
            "3663/3663 [==============================] - 881s 240ms/step - loss: 0.8680 - val_loss: 0.9122\n",
            "Epoch 8/10\n",
            "3663/3663 [==============================] - 880s 240ms/step - loss: 0.8408 - val_loss: 0.9009\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "tical and statistical performance on the problem of the proposed algorithm is a powerful approach to the problem of finding a single expected convex o\n",
            "\n",
            "uence of the proposed algorithm for the standard problem with a previous state-of-the-art performance on the standard method that are similar to the s\n",
            "\n",
            "of the proposed method to support the state-of-the-art methods to approximate the state-of-the-art methods that are able to make a simple and strong c\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "proach on the standard inference method that can be applied for developing the theoretical and nearly optimized by a problem and an algorithm for both\n",
            "\n",
            " and variance, this is the function is prior in the algorithm and demonstrate the first experiments on a compact fine-grained condition on the sensory\n",
            "\n",
            "or context of our algorithms to select a constant factor from latent variables, that is able to predict and show that the method can be used to solve \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "ms in natural language using rational objectives. The model provides a normally, we consider a sequence of the analysis also lower bounds on a partiti\n",
            "\n",
            "also to exponar Dirichlet allocation algorithms for binary spike in the acceleration. Finally, we build the linear regression in a deep learning rate \n",
            "\n",
            "imates an exponential interruption. Experiments demonstrate the top of Wrauting VC, splits for the run truth output to text tabular-vectors on that ca\n",
            "\n",
            "Epoch 9/10\n",
            "3663/3663 [==============================] - 875s 239ms/step - loss: 0.8143 - val_loss: 0.8941\n",
            "Epoch 10/10\n",
            "3663/3663 [==============================] - 881s 241ms/step - loss: 0.7887 - val_loss: 0.8902\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "r the problem of finding a single exploration of the state of the art algorithms that are also presented to achieve a simple and convergence rate of t\n",
            "\n",
            "the data set is a powerful tool for the state-of-the-art performance of the proposed method for the state-of-the-art methods are also provided to achi\n",
            "\n",
            "used to construct a new convergence rate of the proposed method to solve the proposed algorithm to solve the proposed method to predict the training s\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "sh and analyze the problem of estimating the local time series (i.e., faster than the number of actions, and $m$ where $p$ is the $\\ell_1$-regularized\n",
            "\n",
            "eed. We show that this rate has a variant of the set of probabilistic models have been focused on multi-class classification results for the context o\n",
            "\n",
            "od estimator. We also prove that the most probabilistic field procedure for discriminative transformations. We propose a novel analysis of the propose\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "a rigorous statistical linear model. In site expectation, such degrees to model numerous applications, arises under standard Fisher information.\n",
            "\n",
            "On M\n",
            "\n",
            "resenting the marginal ensembles (but also select) optimization problems are scaled the discriminative effects they are dmegance for higher-level-tuni\n",
            "\n",
            "and can be done in the prior work in their elements. In contrast with an intractable ridg degradation. We further highlight projections on deep feed-f\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qp92L4rKx1vI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_abstracts_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_abstracts_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_abstracts_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MbHB02wGW5VT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Papers"
      ]
    },
    {
      "metadata": {
        "id": "NpqZDK4JYBcK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/papers.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddqcjNHFYB86",
        "colab_type": "code",
        "outputId": "1ac92437-17e7-4dea-e2a5-733fcc51c950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# ~x hours to train\n",
        "model_name = 'char_papers'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 58,834,263 character sequences.\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GI-z-JbwbnaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_papers_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_papers_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_papers_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vN5JuIttSOzd"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-Level RNNs"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SRXbdrn4SOze",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 10,\n",
        "    'max_words': 10000,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': True,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 50,\n",
        "    'gen_epochs': 5,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 80,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-B81V_sHorx8"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Paper Abstracts"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bMxSlE16yQMF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/abstracts.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q-I3MvJjyQMQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~ 1 hour to train\n",
        "model_name = 'word_abstracts'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fPqTqfJKyQMa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_abstracts_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_abstracts_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_abstracts_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-tNYQo9XE87",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Papers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wqE8k-bbbbvZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/papers.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bs5ds9Knbbvj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~ 1 hour to train\n",
        "model_name = 'word_papers'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pcp4gG8dbem6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_papers_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_papers_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_papers_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrZSlxDxanjl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}