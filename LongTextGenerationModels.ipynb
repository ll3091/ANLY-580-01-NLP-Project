{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LongTextGenerationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ll3091/ANLY-580-01-NLP-Project/blob/master/LongTextGenerationModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mrB_G9Ab1QR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Project: Text Generation Model Training"
      ]
    },
    {
      "metadata": {
        "id": "PtS7V74X1VaL",
        "colab_type": "code",
        "outputId": "b9f3f188-dc8e-438e-c65e-eea066486592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# source https://github.com/minimaxir/textgenrnn\n",
        "! pip install textgenrnn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textgenrnn in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.8.0)\n",
            "Requirement already satisfied: keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (0.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3QC0MFkv2RJ6",
        "colab_type": "code",
        "outputId": "1098a603-0cd8-4d2e-da99-93c75464b72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from textgenrnn import textgenrnn\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-aqy7vb56W0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# connect to Google drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk5CAFIn6I3D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8yE6_sA8a2P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls gdrive/'My Drive'/NLPProject"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzD5Qs813CyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level RNNs"
      ]
    },
    {
      "metadata": {
        "id": "TvcIyOoA2Usc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 40,\n",
        "    'max_words': 300,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': False,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 10,\n",
        "    'gen_epochs': 2,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 150,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9lbt3CJ3M4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Paper Abstracts"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GpCxwNQNx1u2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/abstracts.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0ol0b6xrx1vC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2380
        },
        "outputId": "81d07317-ec73-4abf-ad5f-72fe2d114c15"
      },
      "cell_type": "code",
      "source": [
        "# ~2.5 hours to train\n",
        "model_name = 'char_abstracts'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 1,123,185 character sequences.\n",
            "Epoch 1/10\n",
            "2193/2193 [==============================] - 529s 241ms/step - loss: 2.3027 - val_loss: 1.2324\n",
            "Epoch 2/10\n",
            "2193/2193 [==============================] - 524s 239ms/step - loss: 1.1585 - val_loss: 1.0909\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "ional and approximate approaches are a single inference and state of the art algorithm for a set of the proposed approach is a general problem is a no\n",
            "\n",
            " and analytical analysis of the surrogate model and a set of data and a set of the problem of the problem of the computational computational analysis.\n",
            "\n",
            "model and a complexity of the computational and experimental results on the noise and approximate model are a graphical problem in the computed at a p\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "state-of-the-art methods for the problem as a problem of data is a single problems.\n",
            "\n",
            "Representation Estimators for Animing Learning from Learning for \n",
            "\n",
            "antial to the general analysis of a general complexity of the noisy information and proposed are state-of-the-art algorithm in a general datasets and \n",
            "\n",
            " large set of the proposed method is a general data. In this paper, we also study the proposed decision model can be also study a large subsencial des\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "; tracences of 9 learning.\n",
            "\n",
            "Learning Surrogation-Observations for Edge Mistrac-Bayesian Desired agents\n",
            "We also novel find by on exploited artifyic max\n",
            "\n",
            " effeclit in machine learning work, we present aim of a coding of proper modRL, in a new each growscon. Experimental noise layer convergence is do thi\n",
            "\n",
            "objected up to parament framework. Approaches and higher-technique results. We provide accognitue the Leib model that presented to frite provided a ri\n",
            "\n",
            "Epoch 3/10\n",
            "2193/2193 [==============================] - 524s 239ms/step - loss: 1.0581 - val_loss: 1.0342\n",
            "Epoch 4/10\n",
            "2193/2193 [==============================] - 523s 238ms/step - loss: 1.0034 - val_loss: 1.0059\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "istical and real data sets are shown that our approach with the statistical analysis of the proposed methods for the statistical and real data sets ar\n",
            "\n",
            " of the art algorithm is a single problem of the statistical analysis of the statistical and real data sets and the results of the proposed algorithm \n",
            "\n",
            "ithm that are the problem of the proposed structure of the general statistical analysis of the proposed algorithm can be used to solve the set of the \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "ic data in a low rank special cases that is generalize the signal threshold of the property of our theory is supervised as a form of the convex optimi\n",
            "\n",
            "edups of the problem of the set of training settings where the first approach on special conditional models. We propose a simple experiments on real d\n",
            "\n",
            "rithm which are better than recovery to experimental results of the problem. While the results of the networks of the sublinear noise ensemble to the \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "orks that goase synthesize samples balance Agabres at the simple augmentation of our theoretical guarantees. As well as composition recent results ind\n",
            "\n",
            "e reinforcement learning which matrix the optimal covariance to a (suite log-like property) as well as a sensory by applied by precive bottomuL under \n",
            "\n",
            "der the step tensor lead. We propose various suite invariant inference in classification, which scan rise the emilarity accumulation subsequent policy\n",
            "\n",
            "Epoch 5/10\n",
            "2193/2193 [==============================] - 522s 238ms/step - loss: 0.9629 - val_loss: 0.9798\n",
            "Epoch 6/10\n",
            "2193/2193 [==============================] - 533s 243ms/step - loss: 0.9279 - val_loss: 0.9655\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "nally efficient and statistical learning algorithms for the computational conditional distributions. The algorithm is a simple state of the art method\n",
            "\n",
            "for Structured Prediction\n",
            "We propose a novel algorithm for the convex optimization problem of the regret bound of the problem of the state-of-the-art \n",
            "\n",
            "oblem in the problem of the state-of-the-art experiments on the computational complexity of the proposed algorithm for the problem of the proposed met\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "arkov chain Monte Carlo algorithm is the goal is to address the complexity of the computational constraint that can obtained a significant improvement\n",
            "\n",
            " can be explored to produce a stationary state space of the underlying corresponding corresponding theoretical guarantees. We also design a novel meth\n",
            "\n",
            "ed on the standard polytope and statistical models for the state of the art contributions of the relationship by a computationally fast and computatio\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "ional Descent (DRMM) and Cameran evented in other players.\n",
            "\n",
            "Reward in Submodular Faces for High-imputed Graphics\n",
            "We study this work fixed by statistic\n",
            "\n",
            "d based recognition settings, for order-of-- object discriminative results. The algorithm applies to zaro insights of these point symmetry outperform \n",
            "\n",
            "nfer them based on an arbitrary state space, we introduce a range of results via {\\em n astrox} and 82-based dramatic minimisution (RNNs) is to valida\n",
            "\n",
            "Epoch 7/10\n",
            "2193/2193 [==============================] - 540s 246ms/step - loss: 0.8962 - val_loss: 0.9511\n",
            "Epoch 8/10\n",
            "2193/2193 [==============================] - 546s 249ms/step - loss: 0.8654 - val_loss: 0.9424\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "stical constraint. The main control of the proposed method is a simple approach to the proposed setting where the proposed method is more efficient an\n",
            "\n",
            "n the state-of-the-art performance on the state-of-the-art performance in the state of the art prediction and the state-of-the-art performance on the \n",
            "\n",
            "e-art performance on the state-of-the-art methods are a simple and provide a simple posterior inference algorithm for the proposed method and show tha\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "e statistical constraint of the human problem size, a new data set with different space of sensors from the fact that the state of the art object reco\n",
            "\n",
            "f the recent work in the price of the state-of-the-art results show that the temporal results on the environment of the computational and spatial netw\n",
            "\n",
            "the complexity of these samples can be deployed by based on a set of spectral models demonstrate the effectiveness of the decision process for the mos\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "r are small environments when and subscale with spatiotemporal discrete matrix and i.i.d. algorithm achieves scalable generalization locational effect\n",
            "\n",
            "stical guarantee for decoder and ImageNet,gives. The proposed Newton's Laplace model and so-called Least Squares Independent Detectors On the Word-Mem\n",
            "\n",
            " chain Monte Carlo (MPR). For random haman consistently by eacout non-model spectrum. This is regret in termeron distinguishes, and a proper selection\n",
            "\n",
            "Epoch 9/10\n",
            "2193/2193 [==============================] - 546s 249ms/step - loss: 0.8359 - val_loss: 0.9342\n",
            "Epoch 10/10\n",
            " 945/2193 [===========>..................] - ETA: 4:41 - loss: 0.8072"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qp92L4rKx1vI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_abstracts_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_abstracts_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_abstracts_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MbHB02wGW5VT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Papers"
      ]
    },
    {
      "metadata": {
        "id": "NpqZDK4JYBcK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/papers.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddqcjNHFYB86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~x hours to train\n",
        "model_name = 'char_papers'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GI-z-JbwbnaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_papers_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_papers_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_papers_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vN5JuIttSOzd"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-Level RNNs"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SRXbdrn4SOze",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 10,\n",
        "    'max_words': 10000,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': True,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 50,\n",
        "    'gen_epochs': 5,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 80,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-B81V_sHorx8"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Paper Abstracts"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bMxSlE16yQMF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/abstracts.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q-I3MvJjyQMQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~ 1 hour to train\n",
        "model_name = 'word_abstracts'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fPqTqfJKyQMa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_abstracts_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_abstracts_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_abstracts_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-tNYQo9XE87",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Papers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wqE8k-bbbbvZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/papers.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bs5ds9Knbbvj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~ 1 hour to train\n",
        "model_name = 'word_papers'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pcp4gG8dbem6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_papers_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_papers_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_papers_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrZSlxDxanjl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}