{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LongTextGenerationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ll3091/ANLY-580-01-NLP-Project/blob/master/LongTextGenerationModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mrB_G9Ab1QR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Project: Text Generation Model Training"
      ]
    },
    {
      "metadata": {
        "id": "PtS7V74X1VaL",
        "colab_type": "code",
        "outputId": "b9f3f188-dc8e-438e-c65e-eea066486592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# source https://github.com/minimaxir/textgenrnn\n",
        "! pip install textgenrnn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textgenrnn in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.8.0)\n",
            "Requirement already satisfied: keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (0.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py->textgenrnn) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3QC0MFkv2RJ6",
        "colab_type": "code",
        "outputId": "1098a603-0cd8-4d2e-da99-93c75464b72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from textgenrnn import textgenrnn\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-aqy7vb56W0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# connect to Google drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk5CAFIn6I3D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8yE6_sA8a2P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls gdrive/'My Drive'/NLPProject"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzD5Qs813CyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level RNNs"
      ]
    },
    {
      "metadata": {
        "id": "TvcIyOoA2Usc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 40,\n",
        "    'max_words': 300,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': False,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 10,\n",
        "    'gen_epochs': 2,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 150,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9lbt3CJ3M4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Paper Abstracts"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GpCxwNQNx1u2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/abstracts.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0ol0b6xrx1vC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2856
        },
        "outputId": "81d07317-ec73-4abf-ad5f-72fe2d114c15"
      },
      "cell_type": "code",
      "source": [
        "# ~2.5 hours to train\n",
        "model_name = 'char_abstracts'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 1,123,185 character sequences.\n",
            "Epoch 1/10\n",
            "2193/2193 [==============================] - 529s 241ms/step - loss: 2.3027 - val_loss: 1.2324\n",
            "Epoch 2/10\n",
            "2193/2193 [==============================] - 524s 239ms/step - loss: 1.1585 - val_loss: 1.0909\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "ional and approximate approaches are a single inference and state of the art algorithm for a set of the proposed approach is a general problem is a no\n",
            "\n",
            " and analytical analysis of the surrogate model and a set of data and a set of the problem of the problem of the computational computational analysis.\n",
            "\n",
            "model and a complexity of the computational and experimental results on the noise and approximate model are a graphical problem in the computed at a p\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "state-of-the-art methods for the problem as a problem of data is a single problems.\n",
            "\n",
            "Representation Estimators for Animing Learning from Learning for \n",
            "\n",
            "antial to the general analysis of a general complexity of the noisy information and proposed are state-of-the-art algorithm in a general datasets and \n",
            "\n",
            " large set of the proposed method is a general data. In this paper, we also study the proposed decision model can be also study a large subsencial des\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "; tracences of 9 learning.\n",
            "\n",
            "Learning Surrogation-Observations for Edge Mistrac-Bayesian Desired agents\n",
            "We also novel find by on exploited artifyic max\n",
            "\n",
            " effeclit in machine learning work, we present aim of a coding of proper modRL, in a new each growscon. Experimental noise layer convergence is do thi\n",
            "\n",
            "objected up to parament framework. Approaches and higher-technique results. We provide accognitue the Leib model that presented to frite provided a ri\n",
            "\n",
            "Epoch 3/10\n",
            "2193/2193 [==============================] - 524s 239ms/step - loss: 1.0581 - val_loss: 1.0342\n",
            "Epoch 4/10\n",
            "2193/2193 [==============================] - 523s 238ms/step - loss: 1.0034 - val_loss: 1.0059\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "istical and real data sets are shown that our approach with the statistical analysis of the proposed methods for the statistical and real data sets ar\n",
            "\n",
            " of the art algorithm is a single problem of the statistical analysis of the statistical and real data sets and the results of the proposed algorithm \n",
            "\n",
            "ithm that are the problem of the proposed structure of the general statistical analysis of the proposed algorithm can be used to solve the set of the \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "ic data in a low rank special cases that is generalize the signal threshold of the property of our theory is supervised as a form of the convex optimi\n",
            "\n",
            "edups of the problem of the set of training settings where the first approach on special conditional models. We propose a simple experiments on real d\n",
            "\n",
            "rithm which are better than recovery to experimental results of the problem. While the results of the networks of the sublinear noise ensemble to the \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "orks that goase synthesize samples balance Agabres at the simple augmentation of our theoretical guarantees. As well as composition recent results ind\n",
            "\n",
            "e reinforcement learning which matrix the optimal covariance to a (suite log-like property) as well as a sensory by applied by precive bottomuL under \n",
            "\n",
            "der the step tensor lead. We propose various suite invariant inference in classification, which scan rise the emilarity accumulation subsequent policy\n",
            "\n",
            "Epoch 5/10\n",
            "2193/2193 [==============================] - 522s 238ms/step - loss: 0.9629 - val_loss: 0.9798\n",
            "Epoch 6/10\n",
            "2193/2193 [==============================] - 533s 243ms/step - loss: 0.9279 - val_loss: 0.9655\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "nally efficient and statistical learning algorithms for the computational conditional distributions. The algorithm is a simple state of the art method\n",
            "\n",
            "for Structured Prediction\n",
            "We propose a novel algorithm for the convex optimization problem of the regret bound of the problem of the state-of-the-art \n",
            "\n",
            "oblem in the problem of the state-of-the-art experiments on the computational complexity of the proposed algorithm for the problem of the proposed met\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "arkov chain Monte Carlo algorithm is the goal is to address the complexity of the computational constraint that can obtained a significant improvement\n",
            "\n",
            " can be explored to produce a stationary state space of the underlying corresponding corresponding theoretical guarantees. We also design a novel meth\n",
            "\n",
            "ed on the standard polytope and statistical models for the state of the art contributions of the relationship by a computationally fast and computatio\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "ional Descent (DRMM) and Cameran evented in other players.\n",
            "\n",
            "Reward in Submodular Faces for High-imputed Graphics\n",
            "We study this work fixed by statistic\n",
            "\n",
            "d based recognition settings, for order-of-- object discriminative results. The algorithm applies to zaro insights of these point symmetry outperform \n",
            "\n",
            "nfer them based on an arbitrary state space, we introduce a range of results via {\\em n astrox} and 82-based dramatic minimisution (RNNs) is to valida\n",
            "\n",
            "Epoch 7/10\n",
            "2193/2193 [==============================] - 540s 246ms/step - loss: 0.8962 - val_loss: 0.9511\n",
            "Epoch 8/10\n",
            "2193/2193 [==============================] - 546s 249ms/step - loss: 0.8654 - val_loss: 0.9424\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "stical constraint. The main control of the proposed method is a simple approach to the proposed setting where the proposed method is more efficient an\n",
            "\n",
            "n the state-of-the-art performance on the state-of-the-art performance in the state of the art prediction and the state-of-the-art performance on the \n",
            "\n",
            "e-art performance on the state-of-the-art methods are a simple and provide a simple posterior inference algorithm for the proposed method and show tha\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "e statistical constraint of the human problem size, a new data set with different space of sensors from the fact that the state of the art object reco\n",
            "\n",
            "f the recent work in the price of the state-of-the-art results show that the temporal results on the environment of the computational and spatial netw\n",
            "\n",
            "the complexity of these samples can be deployed by based on a set of spectral models demonstrate the effectiveness of the decision process for the mos\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "r are small environments when and subscale with spatiotemporal discrete matrix and i.i.d. algorithm achieves scalable generalization locational effect\n",
            "\n",
            "stical guarantee for decoder and ImageNet,gives. The proposed Newton's Laplace model and so-called Least Squares Independent Detectors On the Word-Mem\n",
            "\n",
            " chain Monte Carlo (MPR). For random haman consistently by eacout non-model spectrum. This is regret in termeron distinguishes, and a proper selection\n",
            "\n",
            "Epoch 9/10\n",
            "2193/2193 [==============================] - 546s 249ms/step - loss: 0.8359 - val_loss: 0.9342\n",
            "Epoch 10/10\n",
            "2193/2193 [==============================] - 542s 247ms/step - loss: 0.8078 - val_loss: 0.9326\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "ters of the proposed algorithm achieves a simple and efficient algorithm that enjoys the regret lower bound on the standard regression and segmentatio\n",
            "\n",
            "nd statistical and statistical accuracy of the proposed method to design an efficient algorithm that allows for the state-of-the-art results on the pr\n",
            "\n",
            "rd privacy compared to the problem of the proposed method is to be seen as a subset of the model to solve the distribution over the state-of-the-art m\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "he revenue sharing rules in the data acquisition and variables in a neural network (e.g., the context of an end-to-end framework for explicit and stro\n",
            "\n",
            " achieve a continuous-time series of the proposed algorithm on a target variable from a single regression of the data. We formalize the experimental s\n",
            "\n",
            "steps or sequences in deep neural networks to solve the context of the setting where the loss function is the first convex optimization problem. The p\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "the framework, we present a hierarchical Bayesian optima. These results over a GAN syng5 for very positive modeling. We propose novel features and uti\n",
            "\n",
            "le the framework. We evaluate a set of Stein Bandits Approach\n",
            "Outperform programming frameworks that not only such those follow and safety updating co\n",
            "\n",
            "e Completion noisy search as we show that CONN term used in reproducing kernel Hilbert space and observable, PCA hold through associated large-scale l\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qp92L4rKx1vI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_abstracts_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_abstracts_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_abstracts_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MbHB02wGW5VT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Papers"
      ]
    },
    {
      "metadata": {
        "id": "NpqZDK4JYBcK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1200+ papers would take over 30 hours to train, use a subset\n",
        "file = './gdrive/My Drive/NLPProject/papers50.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddqcjNHFYB86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~x hours to train\n",
        "model_name = 'char_papers50'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GI-z-JbwbnaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_papers50_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_papers50_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_papers50_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vN5JuIttSOzd"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-Level RNNs"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SRXbdrn4SOze",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 10,\n",
        "    'max_words': 10000,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': True,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 50,\n",
        "    'gen_epochs': 5,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 80,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-B81V_sHorx8"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Paper Abstracts"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bMxSlE16yQMF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/abstracts.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q-I3MvJjyQMQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8962
        },
        "outputId": "2fcc4dfe-74db-4985-b051-6e108852ebc2"
      },
      "cell_type": "code",
      "source": [
        "# ~ 1 hour to train\n",
        "model_name = 'word_abstracts'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 198,219 word sequences.\n",
            "Epoch 1/50\n",
            "387/387 [==============================] - 56s 144ms/step - loss: 5.6924 - val_loss: 5.0087\n",
            "Epoch 2/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 4.6190 - val_loss: 4.7371\n",
            "Epoch 3/50\n",
            "387/387 [==============================] - 52s 135ms/step - loss: 4.1298 - val_loss: 4.6795\n",
            "Epoch 4/50\n",
            "387/387 [==============================] - 51s 132ms/step - loss: 3.7161 - val_loss: 4.7331\n",
            "Epoch 5/50\n",
            "387/387 [==============================] - 51s 131ms/step - loss: 3.3400 - val_loss: 4.8387\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "representations from a single image . we propose a novel generative model that operates on the input data . we demonstrate that our model can be used to train neural networks . we propose a novel deep generative model , which can handle rich predictions to produce a model of complex systems . we propose a new method for designing the cross validation data in the original dataset . in addition , we demonstrate that the proposed method can be\n",
            "\n",
            "our algorithm .\n",
            "\n",
            " a general purpose of adaptive submodular functions with lipschitz functions . we give theoretical guarantees for the sample complexity of our algorithm .\n",
            "\n",
            " fast and robust pca via stein variational inference\n",
            "we propose a novel algorithm that aims at a time - step counterpart .\n",
            "\n",
            " learning deep neural networks for sequence - to - sequence models .\n",
            "\n",
            " learning bayesian networks for multi - agent systems\n",
            "we propose\n",
            "\n",
            "representations from unlabeled video frames . we demonstrate the effectiveness of our approach on several benchmark datasets .\n",
            "\n",
            " lifelong learning with symmetric linear models\n",
            "we propose a novel algorithm that learns a joint distribution of the data matrix . we propose a simple and modularized dual - augmented lagrangian algorithm for the purpose of min - max propagation\n",
            "we propose a novel algorithm that learns a joint distribution of the data matrix . we propose a\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "on the standard uni - task learning problem .\n",
            "\n",
            " learning with relaxed objectives\n",
            "the dueling bandit problem is a widely used method for learning the prediction task in the online learning setting . in addition , we show that the proposed method can achieve optimal regret bounds on the misclassification rate of the original subspace . we demonstrate that our framework can be used to approximate the posterior distribution . we establish a novel algorithm for the\n",
            "\n",
            "to match both the full - information between - output kernel and a new form of data . we present a new algorithm for the banditsmooth , where the optimum is a sum of structured output . we then apply our framework to the generative model of dirichlet allocation and real - world datasets .\n",
            "\n",
            " a bayesian model of interpretable and interpretable neural connectivity from hyper - parameter dynamic latent spaces\n",
            "we propose a convex - concave\n",
            "\n",
            "in the two - step domain adaptation . we propose a novel estimator , unchainedbandits , and enables a robust $ s $ - norm square loss function , and the estimation error bounds are universal , with a good approximation guarantee . in addition , we demonstrate that our approach seems to the state - of - the - art performance of interior - point methods , with almost no regret , and provide a truly practical implementation of\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "optimal data - - ignore under ( otherwise ) are a special case of s own - length . to handle consistency results , we introduce and realistic conditions on using a non - negative decomposable policy , which overcomes the univariate euclidean space is a proper bayesian model of cdf in social networks game . poo ' impact on these sensors are dpps by combining textual pixels . ideally , accurate data applicable becomes a distribution of binary classification\n",
            "\n",
            "exponential in high - order data capsules . estimating certain one hand , arms can can be understood as better suited to problems where heuristic are often used in automatic speech recognition systems with complex machine translation . we show that under mild conditions , it leads to interpolate unhinged under high - order ? pomdp , yielding a top - down acquiring dropout method per - all . however , the discriminator , additional gives rise to a little\n",
            "\n",
            "with missing bases in graphs\n",
            "linear dynamical systems are described using viral vertex cover\n",
            "nowozin , approximated path from 3d information diffusion and neural machine translation\n",
            "we introduce a method that operates at the intersection of ica and vice versa . remarkable empirical studies have shown to provide satisfactory theoretical analysis ( measured . equivalently , our dose - response functions are needed to predict the observations of each training samples . the experimental results are made\n",
            "\n",
            "Epoch 6/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 3.0026 - val_loss: 5.0006\n",
            "Epoch 7/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 2.7098 - val_loss: 5.1287\n",
            "Epoch 8/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 2.4624 - val_loss: 5.2663\n",
            "Epoch 9/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 2.2513 - val_loss: 5.3912\n",
            "Epoch 10/50\n",
            "387/387 [==============================] - 51s 131ms/step - loss: 2.0718 - val_loss: 5.5073\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "three problems : given a statistical learning rate of $ n $ - means clustering , and ( 3 ) psm ( with more than lstms ) than other baselines .\n",
            "\n",
            " improved training of gans with neural networks\n",
            "we present a new way to train convolutional neural networks ( cnns ) . this paper proposes a paradigm for understanding physical scenes with images , which are the task of finding collision - free , but rather than\n",
            "\n",
            "a number of classifiers in order to add further information about the target distribution . the new algorithm is to treat integration and discrete synapses . we propose a novel technique for edge reduction in neural networks with comparisons\n",
            "we consider the problem of learning a low rank matrix $ x _ { ab } of $ x _ { r } ^ { n\\times d } $ . we demonstrate the superiority of our approach on both synthetic\n",
            "\n",
            "a number of classifiers in order to add further information about the target distribution . we propose a new algorithm for edge inhomogeneity . we demonstrate the efficacy of this approach on a number of classifiers in order to add further information about the target distribution . we propose a novel algorithm that uses newton ' s method to solve the internal covariate shift . the resulting model can be equivalently formulated as a nonlinear dynamical system . the resulting\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "bandit setting of the standard zero - sum problems . we propose a cutting - aware algorithm that uses this class to compute a non - linear objective function , where a collection of being functions is a major challenge due to its many borderline independence , and thus the task is not learning to be a weighted subset of objects , whereas when the human is pruned . it is that the adversarial discriminator can be solved effectively ,\n",
            "\n",
            "the problems of detecting multi - class classification ( e . g . , multisets of intervals ) . we propose a generative model for pure - distributed sparse regression ( dynamic ) . in this paper , we propose a novel method for parallelizing bayesian logistic regression with convolutional neural networks ( cnn ) . the former introduces of exact sampling to approximate the estimation error . we then show how this estimator can learn to make a more\n",
            "\n",
            "gradient descent ( pg algorithm ) is a powerful generalization in the online probabilistic model ( hmms ) framework for bayesian nonparametric ( graphical models . our model is a variational generalization of the encoder - decoder framework to incentivize uncertainty in the raw system , and thus the weight vector is generated optimistically . we demonstrate the efficacy of our approach on the popular popular semi - critic ( qa ) algorithm . we also discuss the ways of\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "adapted content lagrangian to model the identification of a rigid dynamics .\n",
            "\n",
            " decreasing lstm : a chinese voting task\n",
            "in this paper , we present the first time algorithm in theory about twice of those of the data . by using such oracles genie : ( 1 ? + cumulative ) computes hierarchical sparsity ; ( 2 ) an expanded feature exploitation ( p ) and rank $ matrix $ differ by $ w $ ) ,\n",
            "\n",
            "a tractable risk estimator which may be the possibility to computationally dynamic programming ( by multiple networks ) . the recommender system is performed by the largest classes of sequentially ratings , but are known to be seen as more complete . we overcome this question by introducing reparameterization gradients to the instability computation . this paper proposes an embedding model , called the row value of an internal model ( x ) to define proper ( u - unequal\n",
            "\n",
            "furthermore , it often known for dimensionality reduction algorithms , with small weights for constant dimensions and group sparsity . we improve the consequences of the new framework by employing temporally extended \\emph { truncated search } ) , with an integer linear phase transition probabilities can help understand the benefits of locality vs non - uniform sampling , computation from both the input data and the marginal likelihood are fully appealing . we apply this framework to neural problems\n",
            "\n",
            "Epoch 11/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.9186 - val_loss: 5.6093\n",
            "Epoch 12/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.7836 - val_loss: 5.7147\n",
            "Epoch 13/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.6689 - val_loss: 5.8178\n",
            "Epoch 14/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.5589 - val_loss: 5.9011\n",
            "Epoch 15/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.4671 - val_loss: 5.9744\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "achieve a more accurate trade - off between statistical accuracy and accuracy . we demonstrate that our method is competitive with other representative gan training on some benchmark datasets .\n",
            "\n",
            " matching neural expectation maximization\n",
            "we propose a new stochastic admm - net method for training generative adversarial networks ( gans ) . using the formalism of smooth functions , we propose a novel incremental variational inference method to learn the properties of the data from historical data\n",
            "\n",
            "to learn local correlations in the final output . we show that the pcmc model is a powerful process of ongoing temporal dynamics to explore cells whose inputs are graphs . we propose a new type of normalizing flow algorithm for training deep neural networks\n",
            "recurrent neural networks ( rnns ) have achieved state - of - the - art performance on standard benchmarks .\n",
            "\n",
            " spals : acceleration and averaging on fairness objectives\n",
            "we study the\n",
            "\n",
            "on multiple problems : 1 .\n",
            "\n",
            " a significant amount of interpretable and configurations for many real datasets .\n",
            "\n",
            " regularizing deep neural networks with approximate sequential representations\n",
            "we propose a method for combining adversarial image representations from video frames using a machine learning approach . we apply these textures to state - of - the - art on several nonsmooth - weighted and negative depth between analytic functions . we show that the proposed model can\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "on multiple joint prediction tasks .\n",
            "\n",
            " sample complexity of episodic fixed - horizon reinforcement learning\n",
            "we consider the problem of bandit optimization with a novel sparse linear dynamical system ( also defined ) . this provides a general approach for learning the treatment effects of local functions , and as scalably as input . in this paper , we show that the joint distribution of the epm hyperparameters disturb the regression task is to separate the network\n",
            "\n",
            "searching policies on the group level . we focus on the reconstruction problem of exploration in the past . we show how to reduce the conditional gradient of the model , and the analysis of the proposed approach are actually just the best - known bounds even if the rewards are heavy - tailed . we also show how to empower these oracles , our model is able to learn a non - secret message within a very simple markov\n",
            "\n",
            "and real - world signals .\n",
            "\n",
            " efficient and flexible inference for deep neural networks\n",
            "this paper presents a new approach for learning the dynamics of active memory and parameter estimation , one can obtain an approximation strictly . we design novel algorithms for the game between the complexity of the graph structure . in this paper , we show that conditioning on the curvature of the estimates , the interval of $ \\beta $ is the number\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "are strong : interact with the restricted same finally , we show that our model can seamlessly balance fairness at train convolutional and pose - and - slab a new autoencoder . our method improves incentives by applying a differentiable saliency for adversarial networks ( variable selection ) . we show that any optimization problem can be trained efficiently in a mini - batch setting . the cv error bound is essentially any suitably : the number of variables needed\n",
            "\n",
            "on binary data from certain regularity assumptions , and then design a new approximation of its many existing method requiring range $ i > > 0 $ times , with $ o ( n ^ { 5 / 4 / 2 } ) and $ l ^ p $ potential model comb on markov random input perturbations . our pre - defined network is a multi - step forward natural generalization in videos of the size , showing that our\n",
            "\n",
            "model of $ f ( x ) \\ | \\ca ( \\theta ) game _ f ( \" viewpoint ) $ , a greedy regularized orthonormal representations and an unlabelled example . minimax optimal results on the top - of - the - wild test problem and learn a generative adversarial network to a new form of compatibility function via a new item . we consider the minimal number of items and \\emph { a } initialization that is used\n",
            "\n",
            "Epoch 16/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.3849 - val_loss: 6.0483\n",
            "Epoch 17/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.3088 - val_loss: 6.1336\n",
            "Epoch 18/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.2389 - val_loss: 6.1896\n",
            "Epoch 19/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 1.1751 - val_loss: 6.2549\n",
            "Epoch 20/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 1.1162 - val_loss: 6.2993\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "a number of tasks , which may be of independent interest .\n",
            "\n",
            " unbounded cache model for neural network\n",
            "generative adversarial networks ( gans ) have shown promise in image generation , and combine information from a large scale helicopter . 3 ) data . we demonstrate the applicability of our proposed method on several state of the art datasets .\n",
            "\n",
            " online linear regression via adaptive sampling\n",
            "we study the fundamental limits of variational inference\n",
            "\n",
            "to a provably learning algorithm that can automatically get consistent estimates of the data and the values of the graph is huge . we show that the identity mapping can be used to additionally improve the scalability of imitation learning approaches to large scale datasets .\n",
            "\n",
            " a unified approach for robust pca via nonconvex optimization and impose constraints . we present an efficient algorithm for the banditsmooth of two convex problems , and prove that the linear estimators\n",
            "\n",
            "multiple joint prediction tasks .\n",
            "\n",
            " sample complexity and summarization in sequential data\n",
            "we develop a general algorithmic building generative model that is able to significantly outperform the learning rates and other time - efficiency . we apply our method to semantic image generation and neural network to perform style transfer . we also evaluate the prototypes selected by a random cover problem , and guarantees the regret of the estimated consistently , with an analogous - memory\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "problem is np - hard in general , but a greedy algorithm can achieve quite good approximation accuracy . we also provide results that derive a novel optimal additive structure for the matrix recovery problem , and show that it is possible to recover a { general high dimensional component $ matrix $ \\tilde { o } ( t ) $ , while the regret of the game scaling is some positive - unlabeled ( u - statistics ) and\n",
            "\n",
            "the popular unsupervised learning task . we analyze the behavior of the gibbs sampler on three examples of a rich set of natural classes , we show that they can recover the sparse parameters of a data distribution and its communities . however , in online advertising it is impossible to learn a model - derived prediction model , and use it to predict confidence from two different features of points and action n , while in a few such\n",
            "\n",
            "a simple and efficient reparametrization of the neural network . the proposed approach combines a ' ' coreset ' approach to that * , we can get an up of second - order information to efficiently optimize the algorithm to a wide range of deterministic and stochastic optimization problems . in addition to deriving this results , we show that the proposed estimation strategy outperforms widely - supervised learning algorithms .\n",
            "\n",
            " fast and flexible monotonic - time learning\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "human community . we abstention with the results for predicting the small sizes of a causal mapping from calcium imaging along our theory . without corruptions , a large class of algorithms can be deployed online to distributed algorithms by solving such problems . they define long as a linear threshold as a top - k loss over examples . we further expand its applicability using synthetic data . experiments on several classification as well as much proposed and show\n",
            "\n",
            "over the envelope i . i . d . ~ samples $ from i , d $ . define a result , it can show how to reduce the output of the nature of a latent state space can image dynamics is common to use in human - gain decisions , in these systems , can recognize only poor local properties of the same as special features of the process with known variation . for such applications , several models\n",
            "\n",
            "an objective function involving that we input it to approximate the confidence bound and study the feed - forward riemannian suffer from the stochastic environment , standard techniques significantly outperformed all competing methods on long - term prediction of search\n",
            "we propose a multi - class bayesian regression model that involves sensitive sampling of input variables . we propose new metric learning ( sail ) that exploit newton ' s axiom , which takes a very limited by one\n",
            "\n",
            "Epoch 21/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 1.0586 - val_loss: 6.3584\n",
            "Epoch 22/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 1.0029 - val_loss: 6.3990\n",
            "Epoch 23/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.9532 - val_loss: 6.4569\n",
            "Epoch 24/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.9127 - val_loss: 6.5164\n",
            "Epoch 25/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.8662 - val_loss: 6.5532\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "data .\n",
            "\n",
            " scaled least squares with a frank - wolfe algorithm for 1 - regularized deep learning\n",
            "we consider a crowdsourcing problem in hierarchical clustering in neural networks . the model can be understood as a stochastic game to handle all a does it leads to extremely generalization performance .\n",
            "\n",
            " variational dropout for deep reinforcement learning\n",
            "we present a scalable algorithm for scalable automated variational inference ( avi ) to provide a powerful generalization\n",
            "\n",
            "a number of real - world data sets .\n",
            "\n",
            " deep neural networks\n",
            "building large - scale gaussian process models have been widely used for efficient and practical applications . however , in the case of comparing ggms , these estimators are often not used in practice . in this paper , we propose a novel collapsed variational inference method to evaluate mcmc algorithms .\n",
            "\n",
            " orthogonal nmf in gibbs sampling\n",
            "we study the problem of\n",
            "\n",
            "learned - based approach can be interpreted as a feature construction , but also obtains good approximation accuracy .\n",
            "\n",
            " sdp relaxation with randomized subspace descent\n",
            "the dueling bandit ( cd ) is a popular strategy of truncated least squares ( g _ linear ) ) ( in the case of the bandit case with a low rank $ m $ of the error of the component gaussians , however , two real - life decisions are interested\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "\\textbf { encoder } ) for application to electronic health records , we show that a network achieves approximate inference of riemann hmc and often with computations .\n",
            "\n",
            " multi - view anomaly detection via measure models\n",
            "we present a scalable algorithm for model selection in sigmoid belief networks . however , such methods can handle fundamental behavior through the domain flow within a multi - agent static model is typically an open problem . our key idea\n",
            "\n",
            "in the sense of the input - output mutual information . to address this issue , we propose a multiscale formulation where the representation data can be uniquely reconstructed as a scaled bregman divergence , a multi - step test is developed , for the resulting non - smooth optimization problem . popular inference algorithms such as belief propagation ( bp ) and generalized belief propagation ( gbp ) and online rank - matrix completion with overlapping constraints . \n",
            "\n",
            "\n",
            "independent image retrieval , including predictive modeling , clearly are faster than state - of - the - art results on the visual qa dataset . last but not only on the dimensional representations , we can therefore handle an analysis of human ball in a neural network . it is trained end - to - end in a single task that perform well to account for the dual variables . also , it is desirable to optimize in the\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "which are used to use in single generality through unifying science , and offer superior performance .\n",
            "\n",
            " combining synthesizing data for efficiency , learning to offers a training procedure and help guide future improvements .\n",
            "\n",
            " on a continuous sgd on synthetic and real datasets\n",
            "\n",
            " sample complexity of learning mahalanobis distance metrics\n",
            "kernel inference seeks a transformation to sample complexity that is equivalent to a factor of 2 t $ d = 3 $\n",
            "\n",
            "box algorithm . it is still not clear enough why many popular optimization strategies at the noise of time and number complexity in cost variance . we provide a general showing that there exist some objective functions are consistently more efficient than existing algorithms , such as the moments do not apply , and the use of dropout could be deployed on their continuous representation , i . e . it can jointly learn a network with known sample complexity\n",
            "\n",
            "this task which can be play a promising approach in recommendation system with a large scale one of the graph . when classes the input is some input , it remains unclear how they cannot be on - or or binary search is we achieve competitive results .\n",
            "\n",
            " o\\left for so - layer compared to torch7 inner resolution optogenetic activity perturbations during training , recognition , and is a key component in an average - associated way .\n",
            "\n",
            "Epoch 26/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 0.8199 - val_loss: 6.5986\n",
            "Epoch 27/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.7811 - val_loss: 6.6353\n",
            "Epoch 28/50\n",
            "387/387 [==============================] - 51s 130ms/step - loss: 0.7468 - val_loss: 6.6699\n",
            "Epoch 29/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.7124 - val_loss: 6.7071\n",
            "Epoch 30/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.6776 - val_loss: 6.7520\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "resulting algorithm achieves better performance on a very large variety of multi - label data and motion capture from various sources . the proposed model can be expressed as a three - fold online learning algorithm . numerical experiments demonstrate that this algorithm performs well on more complex contextual bandits and sparse penalties , two algorithms for clustering and empirical risk minimization : statistical and stochastic descent\n",
            "the k - means clustering problem\n",
            "influence maximization is typically an\n",
            "\n",
            "of both worlds : it is related to prior work on a temporal space of ( online convex optimization ) and \\emph { augmented lagrangian and hamiltonian monte carlo } . specifically , we propose a distributed , flexible nonlinear tensor factorization model over a possibly large external memory . our model is a form of inference in bayesian learning and inference . the markov chain monte carlo procedures that are used are often discrete - time analogues of associated\n",
            "\n",
            "resulting algorithm can achieve any better performance compared to the existing approaches , and we have a substantial gain that the power may be applied to other unsupervised learning approaches . we also discuss the ways by early layers of the deep neural network .\n",
            "\n",
            " spectral methods for submodular value function\n",
            "in this paper , we study the problem of learning representations invariant to the class of distributions . the resulting predictions of the proposed model can\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "location objective over the standard multi - armed bandit problem , we derive a prediction algorithm that is near - optimal with respect to the optimal clustering in which and noise . we apply our algorithm to images the proposed computation and prohibitively showing that our main asymptotic variance for the sbm and matches the lower bound . we also illustrate the efficiency of our results by a concrete realization of asynchronous svrg .\n",
            "\n",
            " subset selection in the\n",
            "\n",
            "performed as observational data . it models settings where tasks is inherently sequential and requires deep multi - scale baseline learning approaches . in particular , we recover a more general version of the marginal log - likelihood . to infer the dependencies within the recent years , recent work have seen a resurgence of interest in bayesian algorithms and learning them , they allow matchingexpectations between two models , but sometimes both important for the field in time -\n",
            "\n",
            "large variety of synthetic examples and one of the function - specific univariate tests .\n",
            "\n",
            " a disentangled representation approach to learn an supervised learning problem . finally , we discuss how our algorithm achieves state of the art performances in the literature . we also show how these operations can be learned on a 9 dof jaco of an image .\n",
            "\n",
            " bayesian optimization for language modeling object regions\n",
            "a fundamental goal in computer vision is\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "in dynamic filter to years , we focus on the problem of filling in all labels . we then show that some basic gradient descent ( agd ) and performing its variants in large - scale since they require standard training into inference and does not require any loss over a target variable while applying a supervised , etc . the algorithm is based on the loop value function , and thus limit simple metric learning functions per - part\n",
            "\n",
            "or sparse , motivated by the mean field , where players are assumed to be drawn uniformly at a training set . using our method , which are a significant step towards alleviating at the levels of a kronecker . they define a game between the true posterior distribution $ p ( x _ 1 ( u ) ) $ for a matrix $ f $ . our algorithm is a simple alternating minimization algorithm which switches between $ \\ell\n",
            "\n",
            "uses the ' ' ' algorithm with a small number of samples . we apply our method to obtain decompositions as the k - means algorithm and give the first - phase completion under such as a natural clustering problem in the presence of noise levels . when the data matrices are also strongly convex , and known that the truncated estimator can achieve the same polynomial to $ o ( \\frac { 1 } { \\sqrt { t }\n",
            "\n",
            "Epoch 31/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.6481 - val_loss: 6.7956\n",
            "Epoch 32/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.6162 - val_loss: 6.8204\n",
            "Epoch 33/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.5887 - val_loss: 6.8482\n",
            "Epoch 34/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.5555 - val_loss: 6.8776\n",
            "Epoch 35/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.5314 - val_loss: 6.9051\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "that we can cast the problem of textual features in a partially observed markov game . the goal of the learner is to minimize the one loss that is not three , we propose a simple reformulation of minoux the kernel svm algorithm , and polynomial these bounds on the gradient of the new erm problem , with a guarantee of data points . we propose an estimate of the singular value decomposition problem when the binary vectors are randomly\n",
            "\n",
            "a more general version of the two widely popular erm problem . we show that the popular jacobi equation using an algorithm with the dynamic training process . the algorithm is based on a distributed system that can be used to estimate the performance of a given data set . in this work , we consider a simple yet important sub - problem : translation from textual descriptions to if - then part is a problem to learn the internal\n",
            "\n",
            "propose a bayesian nonparametric approach that considers both types of correlations via unifying and generalizing with the state - of - the - art high - dimensional prescriptions ( e . g . , \\ ` $ g _ i = | 0 + \\lambda _ d + mn _ d ^ { ( r ) } \\circ \\cdots \\theta ( d ^ { ( r ) } ) $ for a positive semidefinite matrix $ l $ - smooth\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "solution is to minimize the number of experiments on simulated data and then apply our system to the data summarization in a given task and make use the average of the forward system to predict its learning process . in this paper , we consider a class of sdp ' s which is guaranteed to be np - hard . we propose a novel framework for unsupervised feature learning based on bayesian networks . the proposed method is robust to\n",
            "\n",
            "algorithm that is a simple benchmark of $ \\alpha $ - approximate clustering , while achieving satisfactory clustering performance .\n",
            "\n",
            " dynamic safe interruptibility for efficient reinforcement learning\n",
            "we introduce a novel framework to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference in the framework of time series analysis . our upper bound is based on a mean - field objective function which we show that is asymptotically optimal , satisfies a\n",
            "\n",
            "optimal joint regret in linear models . we give convergence rates that depend on the joint distribution $ \\epsilon $ , i . e . , $ g _ 1 $ , here we show that the sample complexity will in fact be the same order as gaussian gaussian distributions . we propose a simple , computationally efficient monte carlo expectation - maximization algorithm for inference on massive data\n",
            "an important problem of network learning is to develop a\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "projections in accelerated stochastic optimization . we show that our methods implicitly adapt to the objective ' s structure distribution . experiments on synthetic data and the real image captioning task , 2015 tasks , and powerful is related to errors that the network is not shared and is comparable to of label samples . furthermore , our scaling outperforms existing algorithms are limited to the choice of the error constraints , allow for model tuning draws from the nonlinear\n",
            "\n",
            "can better reconstruct gradient - based methods . in this paper , we view bbvi with generalized correspondence models ( where we can compute optimal solutions . to observe , in this case , we show that the performance of a recently proposed dynamic program can lead to overfitting and it in computer vision tasks by modeling , where a neural network can be combined with very few measurements .\n",
            "\n",
            " learning theory in markov random fields\n",
            "we\n",
            "\n",
            "sample - optimal and empirically that our algorithms outperform highly - and - achieve state or better than the state - of - the - art quadratic - time kernel - based or energy distance - based tests .\n",
            "\n",
            " variational sparse linear neural network : stochastic variational inference\n",
            "this paper identifies the stochastic estimation approach , to which k players attempt we only solve to the target distribution . it is more refined performs how to the\n",
            "\n",
            "Epoch 36/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 0.5061 - val_loss: 6.9285\n",
            "Epoch 37/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.4834 - val_loss: 6.9529\n",
            "Epoch 38/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.4607 - val_loss: 6.9853\n",
            "Epoch 39/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.4396 - val_loss: 7.0058\n",
            "Epoch 40/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.4183 - val_loss: 7.0285\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "that this error has not the power and can be distributed , it is feasible to assume that incorporating this information will lead to better predictions . we tackle the problem of partitioning the binary classifier on a dictionary of objects and a sequence of local learning problems . our approach is based on computing two objective functions , and present a efficient online algorithm to learn directions of the true posterior .\n",
            "\n",
            " fast second order online learning\n",
            "\n",
            "that this error has not the power and can be difficult to obtain high - quality clusterings for the high - dimensional and low - rank matrix recovery problem , and provably outperforms standard ei by several state - of - the - art methods .\n",
            "\n",
            " using fast weights to attend to the faster convergence property of newton - type methods . the proposed method is easy to implement and significantly outperforms the state of the art .\n",
            "\n",
            "that this error bound is nearly a but lower bound in the bandit setting still in a low - rank matrix . finally unlike existing results that apply these algorithms to popular and natural stimuli such as support vector machines . however , they are notoriously hard to train the question of the objective function .\n",
            "\n",
            " diffusion approximations for online principal component estimation\n",
            "we study the problem of black - box optimization of a function $ n\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "on their corresponding previous observations . we introduce the graphical model , a novel method that allows the target threshold to vary depending on the plausibility of each hypothesis . the current test uses the ' ' coreset ' ' approach with a lower - bound on the number of states needed and analyze its label complexity . we present an algorithm on the analysis of the \\emph { bayesian conditional gradient ( gcg ) - type methods ) in\n",
            "\n",
            "that this model is plausible for mobile health , and a key goal of artificial intelligence from the single image , and the ill - posedness of multiple - play neural networks for automatic induction . in this paper , we introduce recursive maxima hunting ( rmh ) , allowing to infer local structures from images , to adapt to its corresponding setting . we present efficient solutions to illustrate this robust and asymptotic analysis . the method is validated\n",
            "\n",
            "on both synthetic and real datasets .\n",
            "\n",
            " testing and learning on distributions with symmetric noise invariance\n",
            "we consider the problem of active sequential hypothesis testing where a bayesian decision maker must also be used to prune it . while previous work has shown oracles abstaining on specific regression tasks , and we show that while it is possible to recover a { general high probability } $ \\mat a closed - form expression of $ \\mathcal {\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "a general sg - mcmc algorithm referred to adaptive energy learning using stochastic gradient descent to reduce gradient oracle loss and state of the art . we extend the model on computational challenges for the procedure of fixed - context learning\n",
            "we consider a supervised binary convolutional network ( max - min ) unit in training the define and exploits the pac feedback and the number of query complexity in order to the setting , while the objective is\n",
            "\n",
            "of employs mcmc schemes and further provides principled results competitive in classification error which we analytically demonstrate the effectiveness of the denoising - based methods .\n",
            "\n",
            " matrix completion with tensor decomposition\n",
            "we study the problem of completing a binary matrix $ 2d $ with $ n $ data points and $ d $ . the proposed algorithm easily provides a novel statistical and easy to existing empirical convolutions in this direction of common geometry of the geometry\n",
            "\n",
            "and real data , including instructional video summarization demonstrate our method on training image datasets and show that the new unsupervised learning method gives drastically lower and outperforms existing methods .\n",
            "\n",
            " regularized modal regression with applications in learning with applications in optimization and machine learning , we care in learning the joint representations which are beyond favorable sufficiently . by considering the fact that this algorithm is exact recovery , and provide a less separation between the filter\n",
            "\n",
            "Epoch 41/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 0.4003 - val_loss: 7.0389\n",
            "Epoch 42/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.3816 - val_loss: 7.0567\n",
            "Epoch 43/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.3676 - val_loss: 7.0742\n",
            "Epoch 44/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.3476 - val_loss: 7.0893\n",
            "Epoch 45/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.3367 - val_loss: 7.0989\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "\n",
            "\n",
            "learning to transduce with unbounded memory\n",
            "recently , strong results have been studied in the context of deep neural networks , there are various partial monitoring and efficient learners . we argue that the intermediate mapping can be used to make good classification performance . these models can be often used as acoustic and non - pre even when the model is not able to produce samples from the models of the underlying temporal processes , and\n",
            "\n",
            "technique for actively learning the parameters of a mixture model , where the regression weights can be characterized by an expectation . in this paper , we study the problem of f - measure maximization in dynamic environments ( lda ) , which is based on a block coordinate descent ( em ) algorithm . we show that sgd can be learned by that in fact significantly faster than or two stochastic gradient methods . we show that our results\n",
            "\n",
            "\n",
            "\n",
            "learning to transduce with unbounded memory\n",
            "recently , strong results have been studied in the context of deep neural networks , there are various partial monitoring : multi - class classification on regression , linear regression with a sparse gaussian process ( cvp ) that factorizes each of its layers into the product of a connection weight matrix and its nonnegative real - world ( uses to true data ) , including equivalent kernels for which the\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "subset of the data ( k ) of the classifier ' s belief of the empirical objective . we show that our method has a competing approach in practice .\n",
            "\n",
            " learning hierarchical complexity under distance\n",
            "we study the problem of learning the underlying graph of an unknown and intuitive functions , and hence outperform the previous non - private version . we study the effectiveness of our framework to develop a new algorithm that achieves the same\n",
            "\n",
            "\n",
            "\n",
            "deep voice and dense tensor learning\n",
            "a inverse filtering is developed for a learning problem in statistics .\n",
            "\n",
            " online learning with a finite budget : an approximate posterior inference algorithm for ica with gaussian noise and variational auto - encoder mixture models\n",
            "the idea of uprooting and rerooting graphical models was introduced specifically for the joint prediction model . here we introduce the notion of a graph between x and the n $ ,\n",
            "\n",
            "with a differentiable relaxation of the stability of a learned model . in particular , we show that the triplet - consistent polytope tri is unique by a standard training method . in this work , we are interested in generalizing convolutional neural networks ( cnns ) for text categorization via region - based fully supervised clustering . we demonstrate the effectiveness of the proposed model on several popular generative modeling applications . we demonstrate that a natural approach in\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "that can be used to improve the fixed - rank matrix completion problem , where the prediction problem is not agent in limited due . this paper proposes an information - theoretic framework for feature selection and analyze its algorithm\n",
            "we establish upper and lower bounds on the robustness of minimizing rate than others and how to its applicability . we propose a novel sparse formulation , where well known the inference for each recommendation systems are not available\n",
            "\n",
            "to store super - resolution and visual system : we propose a technique to learn bounded vertex statistics as an integer point process . we show that this tensor decomposition accurately approximates the linear fascicle evaluation ( life ) model , one of the recently developed linear models used in a dirichlet process mixture model framework where a subset of points is a fundamental problem in the learning setting where the data is not gaussian and has not limit .\n",
            "\n",
            "further any samples . our method can also be used to also implement the dataset at hand .\n",
            "sprbfaoo , using experimental results for reducing the gap of the proposed method .\n",
            "\n",
            " the marginal process under the observation noise\n",
            "we consider the problem of where a jaccard index m of the columns of $ x $ and diverse weight vectors , and how $ \\gamma $ standard normal . the derived result in contrast recent work\n",
            "\n",
            "Epoch 46/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 0.3207 - val_loss: 7.1079\n",
            "Epoch 47/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.3084 - val_loss: 7.1100\n",
            "Epoch 48/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.2972 - val_loss: 7.1192\n",
            "Epoch 49/50\n",
            "387/387 [==============================] - 50s 129ms/step - loss: 0.2875 - val_loss: 7.1307\n",
            "Epoch 50/50\n",
            "387/387 [==============================] - 50s 130ms/step - loss: 0.2814 - val_loss: 7.1284\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "the resulting algorithm enjoys non - uniform convergence . we show that this algorithm is optimal for a large class of regression problems , which subsumes the full information flow gets of its input , the optimal statistical question is not global . in this paper , we develop a novel estimator for estimating mutual information in discrete - continuous mixtures . we prove that as soon this form , this is the first strong convexity that achieves $ o\n",
            "\n",
            "optimization .\n",
            "\n",
            " a nonconvex optimization method for learning the parameters of a learning problem using both inference in convex optimization . one of the main computational framework of the paper is to guarantee a class of $ \\mathcal { o } ( p ^ { - 1 / 2 } ) $ , were $ t $ is the number of prediction compared to that training , including the first time - aware feature may be used as\n",
            "\n",
            "to undesired behavior ( driving ) . in this paper , we show that the joint alignment problem can be transformed into a network flow problem . in this paper , we develop a bayesian model , wherein agents have differing computational advantages over pairs of data , but are typically trained for all algorithms . in this paper , we develop a bayesian model , wherein agents have differing computational advantages over pairs of data . in particular ,\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "is computationally expensive , and the algorithm can achieve this property , and polynomial mixing the same accuracy is independent . we propose a new class of bounds for this problem , which states that given a set of parameters . it has been shown that iterative dropout procedure not only be deployed in a deterministic or non - smooth setting . instead , we use information - theoretic approach to solve the problem of learning the latent variables to\n",
            "\n",
            "that this error bound is nearly minimax optimal amongst all differentially private algorithms . we apply our approach to the multi - output setting , we show that for the linear regression problem , the proposed algorithm is orders of magnitude faster than other greedy and convex relaxation techniques for learning shift - invariant sparse gaussian processes ( gps ) , which consists of three players . the first is that it can learn to make a complex model but\n",
            "\n",
            "is robust to linear in the worst case . we propose a convex relaxation of the koopman operator to a family of rank - continuous function over $ n $ - length , our algorithm achieves a state - of - the - art performance on standard image modelling benchmarks , can expose latent class representations from a random space or data , and a continuous markov decision process model , which , for the first time the robot arm\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "code to does not lead to overfitting the aim of its signal . in the future\n",
            "we study the problem of active sequential decision making in the parallel computing environment . these methods are efficient even for the offline setting , our methods use the gradient unsupervised methods to illustrate the robustness of training neural networks on several tasks in computer vision , including instance - - a novel analysis of exact filtering . we apply our approach to\n",
            "\n",
            "supervised metric using noisy labeling and side information . the proposed estimator can be used to efficiently optimize the latent path of the model . using the method of generative adversarial nets , this weighted generative model ( sbm ) , for generating graphs in the original graph model . sparsity learning the because of the dropout and the sellers . in this paper , we develop an algorithm that approximates the residual error of tucker decomposition .\n",
            "\n",
            "\n",
            "\n",
            "achieving high test time . we one markov chain monte carlo ( kmc ) is a deterministic strategy that uses random walks to embeddings the performance of a deep multi - task gaussian process ( dmgp ) , the inputs are also not observed . we formulate this new approach to preserve and rank desirable properties that an initialization is on $ \\mathcal { o } ( the n ^ 2 / \\mathbb { c } ) $ properties .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fPqTqfJKyQMa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_abstracts_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_abstracts_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_abstracts_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-tNYQo9XE87",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Papers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wqE8k-bbbbvZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file = './gdrive/My Drive/NLPProject/papers50.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bs5ds9Knbbvj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~ 1 hour to train\n",
        "model_name = 'word_papers50'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pcp4gG8dbem6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_papers50_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_papers50_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_papers50_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrZSlxDxanjl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}