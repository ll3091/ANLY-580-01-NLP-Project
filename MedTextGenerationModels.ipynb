{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MedTextGenerationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ll3091/ANLY-580-01-NLP-Project/blob/master/MedTextGenerationModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mrB_G9Ab1QR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Project: Text Generation Model Training"
      ]
    },
    {
      "metadata": {
        "id": "PtS7V74X1VaL",
        "colab_type": "code",
        "outputId": "65b17811-4ef6-44a6-effa-4766aeaec8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "# source https://github.com/minimaxir/textgenrnn\n",
        "! pip install textgenrnn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textgenrnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/f8/f1968b2078a9076f481916fba5d98affa019943e4f5764224ffaeb57b7c7/textgenrnn-1.4.1.tar.gz (1.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.7MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (0.19.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.14.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.5)\n",
            "Building wheels for collected packages: textgenrnn\n",
            "  Running setup.py bdist_wheel for textgenrnn ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/30/96/f7/bc7042ea671bc79455c244af21050a7a32d604fe2f7a44e322\n",
            "Successfully built textgenrnn\n",
            "Installing collected packages: textgenrnn\n",
            "Successfully installed textgenrnn-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3QC0MFkv2RJ6",
        "colab_type": "code",
        "outputId": "ed6ab01e-2503-4ab8-8c8a-2697ab7d3c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from textgenrnn import textgenrnn\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-aqy7vb56W0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# connect to Google drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk5CAFIn6I3D",
        "colab_type": "code",
        "outputId": "08bee37b-c899-4ac8-f21d-ff1af6dbe1c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data  songs1000.txt  songs100.txt  songs2500.txt  songs500.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8yE6_sA8a2P",
        "colab_type": "code",
        "outputId": "6c6be58e-8102-4528-b43e-79b93d35be3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "! ls gdrive/'My Drive'/NLPProject"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataExploration.ipynb\t\t songdata.csv\n",
            "jokes.txt\t\t\t songs1000.txt\n",
            "MedTextGenerationModels.ipynb\t songs100.txt\n",
            "ModelEvaluation.ipynb\t\t songs2500.txt\n",
            "ModelTrainingOutput\t\t songs500.txt\n",
            "motivational_quotes.txt\t\t TrainedModels\n",
            "ShortTextGenerationModels.ipynb  trump_tweets.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JzD5Qs813CyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level RNNs"
      ]
    },
    {
      "metadata": {
        "id": "TvcIyOoA2Usc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 40,\n",
        "    'max_words': 300,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': False,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 10,\n",
        "    'gen_epochs': 2,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 150,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9lbt3CJ3M4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Song Lyrics"
      ]
    },
    {
      "metadata": {
        "id": "1RyrErwty49Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dir = './gdrive/My Drive/NLPProject/'\n",
        "nums = [100, 500, 1000, 2500]\n",
        "files = [dir+'songs'+str(n)+'.txt' for n in nums]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73Uq0UdOzXBh",
        "colab_type": "code",
        "outputId": "6f348a25-fa76-4425-e131-47a0aa13ee34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "files"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./gdrive/My Drive/NLPProject/songs100.txt',\n",
              " './gdrive/My Drive/NLPProject/songs500.txt',\n",
              " './gdrive/My Drive/NLPProject/songs1000.txt',\n",
              " './gdrive/My Drive/NLPProject/songs2500.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jwR63ffiThLM",
        "outputId": "9367fe6d-2932-475c-aa32-df59c437c4b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        }
      },
      "cell_type": "code",
      "source": [
        "for file, n in zip(files, nums):\n",
        "  print('Training with', str(n), 'songs from', file)\n",
        "  model_name = 'char_'+'songs'+str(n)\n",
        "  textgen = textgenrnn(name=model_name)\n",
        "\n",
        "  train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "  train_function(\n",
        "      file_path=file,\n",
        "      new_model=True,\n",
        "      num_epochs=train_cfg['num_epochs'],\n",
        "      gen_epochs=train_cfg['gen_epochs'],\n",
        "      batch_size=train_cfg['batch_size'],\n",
        "      train_size=train_cfg['train_size'],\n",
        "      dropout=train_cfg['dropout'],\n",
        "      max_gen_length=train_cfg['max_gen_length'],\n",
        "      validation=train_cfg['validation'],\n",
        "      is_csv=train_cfg['is_csv'],\n",
        "      rnn_layers=model_cfg['rnn_layers'],\n",
        "      rnn_size=model_cfg['rnn_size'],\n",
        "      rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "      max_length=model_cfg['max_length'],\n",
        "      dim_embeddings=model_cfg['dim_embeddings'],\n",
        "      word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with 100 songs from ./gdrive/My Drive/NLPProject/songs100.txt\n",
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 94,607 character sequences.\n",
            "Epoch 1/10\n",
            "184/184 [==============================] - 49s 268ms/step - loss: 3.3798 - val_loss: 3.1312\n",
            "Epoch 2/10\n",
            "184/184 [==============================] - 43s 236ms/step - loss: 3.1191 - val_loss: 3.1213\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "                                                                                                                                                      \n",
            "\n",
            " e                                                                                                                     \n",
            "             o                \n",
            "\n",
            "                                                                o     o                                                    eo  a                      \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "   a   a tol h       e   b er  o      e aom v\n",
            "     h  i     ae  h\n",
            "u   \n",
            " o  lr    or  \n",
            " o\n",
            " a  o t   ta  a   u  e  e ma   ty o  n \n",
            " hty y  seo     e t h\n",
            "\n",
            " \n",
            "hed\n",
            " \n",
            "don t   \n",
            "   h h  ttml y    \n",
            "  h    t tT ooa d  e   ar  ttlety      a    l a  sr o     a     t t    tdao uey  on  hh  \n",
            "    t e   t \n",
            "h  a W   s \n",
            "\n",
            "  \n",
            "  ou    yy o  t rau  sooa a ua  e l\n",
            "  to  ae o   a e    \n",
            "   l  e    a nla       t to    l  \n",
            "   llh t   a   y  k    u   t    ot  r o h t\n",
            "d d \n",
            "h  \n",
            "  \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            " iovtRbrovo dueivtdClsmaiay'atth\n",
            " b ysy owd sibleid.tuAlthwotigoelat  odab gynWalohNoc WCerrwt' a\n",
            " luthn\n",
            "\n",
            "v\n",
            "seoyaenloaem N   nTt \n",
            "  e\n",
            " g'agtwhe eetgd\n",
            "\n",
            "\n",
            " douonshs Lhe\n",
            "  yi I t[peh C l ens l  ln  cx'lumeelvtacio own,ea lefh yta\n",
            "uEsoh e  ] 'heb ham' Dep\n",
            "eaaahmetmCs\n",
            "t  hie yednseA y o h\n",
            "m,t?itl'aoubddd'e \n",
            "\n",
            "adte wnu yhef llaaR     p 'roco ss s aoto\n",
            "npahyYhtpuvmue\n",
            "\n",
            " lsp a'\n",
            "de \n",
            "gloutstom o ndw le c eutai\n",
            " ika\n",
            "vgaf h\n",
            "M Reswoei ausu t iagd   euhotw  p hb n(o \n",
            "\n",
            "Epoch 3/10\n",
            "184/184 [==============================] - 44s 236ms/step - loss: 3.1115 - val_loss: 3.1136\n",
            "Epoch 4/10\n",
            "184/184 [==============================] - 43s 236ms/step - loss: 3.1045 - val_loss: 3.1120\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "             o                                                                                         e                                              \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UmDitnT0Ut4V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_songs100_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs100_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs100_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp char_songs500_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs500_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs500_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp char_songs1000_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs1000_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs1000_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp char_songs2500_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs2500_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs2500_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vN5JuIttSOzd"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-Level RNNs"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SRXbdrn4SOze",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 10,\n",
        "    'max_words': 10000,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': True,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 50,\n",
        "    'gen_epochs': 5,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 80,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-B81V_sHorx8"
      },
      "cell_type": "markdown",
      "source": [
        "### Song Lyrics"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Vgdb-CLBQ3TZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for file, n in zip(files, nums):\n",
        "  print('Training with', str(n), 'songs from', file)\n",
        "  model_name = 'word_'+'songs'+str(n)\n",
        "  textgen = textgenrnn(name=model_name)\n",
        "\n",
        "  train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "  train_function(\n",
        "      file_path=file,\n",
        "      new_model=True,\n",
        "      num_epochs=train_cfg['num_epochs'],\n",
        "      gen_epochs=train_cfg['gen_epochs'],\n",
        "      batch_size=train_cfg['batch_size'],\n",
        "      train_size=train_cfg['train_size'],\n",
        "      dropout=train_cfg['dropout'],\n",
        "      max_gen_length=train_cfg['max_gen_length'],\n",
        "      validation=train_cfg['validation'],\n",
        "      is_csv=train_cfg['is_csv'],\n",
        "      rnn_layers=model_cfg['rnn_layers'],\n",
        "      rnn_size=model_cfg['rnn_size'],\n",
        "      rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "      max_length=model_cfg['max_length'],\n",
        "      dim_embeddings=model_cfg['dim_embeddings'],\n",
        "      word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lpd_C0cXQ3Tq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_songs100_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs100_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs100_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp word_songs500_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs500_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs500_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp word_songs1000_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs1000_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs1000_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp word_songs2500_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs2500_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs2500_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}