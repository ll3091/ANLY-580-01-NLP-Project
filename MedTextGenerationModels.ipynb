{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MedTextGenerationModels.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ll3091/ANLY-580-01-NLP-Project/blob/master/MedTextGenerationModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mrB_G9Ab1QR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Project: Text Generation Model Training"
      ]
    },
    {
      "metadata": {
        "id": "PtS7V74X1VaL",
        "colab_type": "code",
        "outputId": "517e5a61-8e08-4e7e-d75d-a98a7fbc12d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "# source https://github.com/minimaxir/textgenrnn\n",
        "! pip install textgenrnn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textgenrnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/f8/f1968b2078a9076f481916fba5d98affa019943e4f5764224ffaeb57b7c7/textgenrnn-1.4.1.tar.gz (1.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.7MB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from textgenrnn) (0.19.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.1.5->textgenrnn) (1.14.6)\n",
            "Building wheels for collected packages: textgenrnn\n",
            "  Running setup.py bdist_wheel for textgenrnn ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/30/96/f7/bc7042ea671bc79455c244af21050a7a32d604fe2f7a44e322\n",
            "Successfully built textgenrnn\n",
            "Installing collected packages: textgenrnn\n",
            "Successfully installed textgenrnn-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3QC0MFkv2RJ6",
        "colab_type": "code",
        "outputId": "8dd6b8a5-29f2-4e50-e270-ac3ed6a12272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from textgenrnn import textgenrnn\n",
        "from google.colab import drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-aqy7vb56W0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# connect to Google drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fk5CAFIn6I3D",
        "colab_type": "code",
        "outputId": "4526171c-ab40-40e2-cd4e-8bf4e18d91e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8yE6_sA8a2P",
        "colab_type": "code",
        "outputId": "1409fee4-74e2-4977-a660-5eb84c66abfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "! ls gdrive/'My Drive'/NLPProject"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataExploration.ipynb\t\t songdata.csv\n",
            "jokes.txt\t\t\t songs10000.txt\n",
            "MedTextGenerationModels.ipynb\t songs1000.txt\n",
            "ModelEvaluation.ipynb\t\t songs25000.txt\n",
            "ModelTrainingOutput\t\t songs5000.txt\n",
            "motivational_quotes.txt\t\t TrainedModels\n",
            "ShortTextGenerationModels.ipynb  trump_tweets.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JzD5Qs813CyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level RNNs"
      ]
    },
    {
      "metadata": {
        "id": "TvcIyOoA2Usc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 40,\n",
        "    'max_words': 300,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': False,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 10,\n",
        "    'gen_epochs': 2,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 150,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9lbt3CJ3M4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Song Lyrics"
      ]
    },
    {
      "metadata": {
        "id": "1RyrErwty49Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dir = './gdrive/My Drive/NLPProject/'\n",
        "nums = [1000, 5000, 10000, 25000]\n",
        "files = [dir+'songs'+str(n)+'.txt' for n in nums]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73Uq0UdOzXBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9fbbce58-e463-4cd2-c5a2-e89dc4a578b5"
      },
      "cell_type": "code",
      "source": [
        "files"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./gdrive/My Drive/NLPProject/songs1000.txt',\n",
              " './gdrive/My Drive/NLPProject/songs5000.txt',\n",
              " './gdrive/My Drive/NLPProject/songs10000.txt',\n",
              " './gdrive/My Drive/NLPProject/songs25000.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "a8ko9a1y44AO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "file = files[i]\n",
        "\n",
        "model_name = 'char_'+'songs'+str(nums[i])\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Xy0mvA430sc",
        "colab_type": "code",
        "outputId": "d7a32d01-9015-4f48-883f-b8e3a164e414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7786
        }
      },
      "cell_type": "code",
      "source": [
        "# ~75 mins to train\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 965,874 character sequences.\n",
            "Epoch 1/10\n",
            "1886/1886 [==============================] - 448s 237ms/step - loss: 1.8757 - val_loss: 1.4698\n",
            "Epoch 2/10\n",
            "1886/1886 [==============================] - 443s 235ms/step - loss: 1.4108 - val_loss: 1.3412\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "hame  \n",
            "  \n",
            "I'm a back and the sea  \n",
            "  \n",
            "I can be a start  \n",
            "I'm a side  \n",
            "  \n",
            "I want to see the shorely  \n",
            "  \n",
            "I was a stray  \n",
            "  \n",
            "I'm a side  \n",
            "I was a part o\n",
            "\n",
            "y to see the same  \n",
            "  \n",
            "I'm gonna be a straight  \n",
            "  \n",
            "And I'm a side  \n",
            "I was a fool of the same  \n",
            "  \n",
            "I'm a straight  \n",
            "  \n",
            "So many can the shade  \n",
            "I say t\n",
            "\n",
            " a sit of the same  \n",
            "  \n",
            "I'm a straight  \n",
            "When I can see the same  \n",
            "  \n",
            "I'm a long of the same  \n",
            "I'm a strange  \n",
            "  \n",
            "I was a lot of the way  \n",
            "  \n",
            "I say to\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "e your heart  \n",
            "So the heart  \n",
            "When I'm all right  \n",
            "  \n",
            "I'm so good of the shoreles  \n",
            "And that you want to hear the time  \n",
            "And if you love you  \n",
            "  \n",
            "So r\n",
            "\n",
            " be a moon  \n",
            "  \n",
            "You see  \n",
            "And the soul  \n",
            "I'm the same of dreamer  \n",
            "And I'm a street  \n",
            "I can see your man  \n",
            "  \n",
            "Cause you said I wish a man  \n",
            "That you'l\n",
            "\n",
            " don't share it's a street  \n",
            "I'm gonna be a love and now  \n",
            "  \n",
            "[Chorus]  \n",
            "  \n",
            "When I can be a white  \n",
            "  \n",
            "What hey this things to hold  \n",
            "  \n",
            "The love of t\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "nk, you gonna get a longh  \n",
            "Make cturn fine adyday  \n",
            "  \n",
            "Yes I meet not cart of love, says you slever  \n",
            "Sweet to leal time  \n",
            "I'm on, looks none  \n",
            "Then \n",
            "\n",
            "pear for me  \n",
            "Turn, not for but you feel your eyes  \n",
            "To this live thenin' his fine  \n",
            "So what you're left to help and  \n",
            "Somethings  \n",
            "And I'm inside jus\n",
            "\n",
            "lly get mi canca a-lonely days when you preb  \n",
            "I've learn away  \n",
            "When I pretting all of better not now  \n",
            "Your way see I don't my heat  \n",
            "And be they're\n",
            "\n",
            "Epoch 3/10\n",
            "1886/1886 [==============================] - 444s 236ms/step - loss: 1.3088 - val_loss: 1.2864\n",
            "Epoch 4/10\n",
            "1886/1886 [==============================] - 443s 235ms/step - loss: 1.2438 - val_loss: 1.2468\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            " be the rain  \n",
            "I want to see the way that I can't be  \n",
            "  \n",
            "I want to see the beat the way  \n",
            "I want to get the stranger  \n",
            "I can't be the soul  \n",
            "I'm a lo\n",
            "\n",
            "t  \n",
            "I want to see you  \n",
            "  \n",
            "I want to see the same  \n",
            "I want to be a crazy  \n",
            "I want to be the same  \n",
            "I can't get the first time  \n",
            "I want to be the feeli\n",
            "\n",
            "nt to be a million  \n",
            "I want to see the world  \n",
            "I want to be the way  \n",
            "I can't find the star  \n",
            "I can't be waiting for the country  \n",
            "There are falling  \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "\n",
            "The stars for a river  \n",
            "I can't be the shade  \n",
            "I found the been here  \n",
            "I'm a trail in a friend  \n",
            "And in the world  \n",
            "I want to hear you make it  \n",
            "I wa\n",
            "\n",
            " well really want to call me  \n",
            "I want to get enough  \n",
            "I know it's me again  \n",
            "  \n",
            "I want to see me thinking for you  \n",
            "  \n",
            "I want to say that was the worl\n",
            "\n",
            "fore your face  \n",
            "  \n",
            "I can't find the bells  \n",
            "I'm a reason too much time  \n",
            "And in the sky  \n",
            "I will always need you  \n",
            "I can tell the road  \n",
            "I want to ro\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "ou do you want complete him it's over again hair  \n",
            "Until the price sucked blue for the muddens and copsure  \n",
            "I hope to it's been, we've dreams  \n",
            "I'm a\n",
            "\n",
            "ng to call me like over mistake.  \n",
            "Speaked forever, cight is a tworing move  \n",
            "When the things, the day only cant telled you  \n",
            "And now you're still smi\n",
            "\n",
            "n Guide-\n",
            "I'm orange really, please?\n",
            "\n",
            "\n",
            "\n",
            "-Regger by Ratta Jand-\n",
            "A shame, some now  \n",
            "I'm symphing for that love will saw  \n",
            "  \n",
            "Without you through, gone K\n",
            "\n",
            "Epoch 5/10\n",
            "1886/1886 [==============================] - 444s 236ms/step - loss: 1.1928 - val_loss: 1.2227\n",
            "Epoch 6/10\n",
            "1886/1886 [==============================] - 445s 236ms/step - loss: 1.1475 - val_loss: 1.1950\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "ay you don't want to see  \n",
            "I want to get the moon  \n",
            "I was born to be  \n",
            "I want to be a chance  \n",
            "The same old stereo soldier  \n",
            "We are the same  \n",
            "The sam\n",
            "\n",
            "same old stereo  \n",
            "  \n",
            "I want to see you  \n",
            "  \n",
            "I want to be a chance  \n",
            "I want to get the street  \n",
            "  \n",
            "I got to be a bad boy  \n",
            "I want to be a chance  \n",
            "I wa\n",
            "\n",
            " was a real good time  \n",
            "I don't want to get a little bit  \n",
            "I don't want to be a second  \n",
            "I don't want to be  \n",
            "I was a super cop  \n",
            "So I know what I was\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "onna stay  \n",
            "Close! (Now)  \n",
            "  \n",
            "(when I'll be and Rally)  \n",
            "  \n",
            "So many time you say that you want me  \n",
            "I want to see me alive  \n",
            "Now I feel all the blood \n",
            "\n",
            "n't wanna be looking for  \n",
            "I don't want to get it  \n",
            "I want to be the shadows are shooting and sing  \n",
            "So with you that I'm gonna be  \n",
            "When I told me  \n",
            "\n",
            "\n",
            "think you got to be the street  \n",
            "The world will be the only one  \n",
            "But it's automatic  \n",
            "But I'm just enough  \n",
            "I can't be your wine  \n",
            "And the way you wa\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            " you always won't control  \n",
            "  \n",
            "One day you wish I pretties  \n",
            "'Cause you never work for you  \n",
            "  \n",
            "I can't smell the radio boots  \n",
            "And I say go 'head  \n",
            "(\n",
            "\n",
            "  \n",
            "And I'm only our money  \n",
            "Off you.  \n",
            "You got it if you feel like what I do you  \n",
            "I've should pretty but I got hurt, it would be bad  \n",
            "Gonna tell if \n",
            "\n",
            "ican  \n",
            "We are the only one you go, buckf here life 'til the  \n",
            "And beautal again and a show  \n",
            "  \n",
            "\"Kuh to never say  \n",
            "So fast  \n",
            "  \n",
            "[Because I'm forget M\n",
            "\n",
            "Epoch 7/10\n",
            "1886/1886 [==============================] - 442s 235ms/step - loss: 1.1056 - val_loss: 1.1762\n",
            "Epoch 8/10\n",
            "1886/1886 [==============================] - 441s 234ms/step - loss: 1.0667 - val_loss: 1.1619\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "reet  \n",
            "I want to see the same  \n",
            "  \n",
            "I will always love you  \n",
            "  \n",
            "I wanna be a chance  \n",
            "I want to see  \n",
            "I want to see the same  \n",
            "  \n",
            "I want to see the str\n",
            "\n",
            " for a love  \n",
            "I don't want to stay  \n",
            "  \n",
            "[Chorus]  \n",
            "  \n",
            "I want to see the same  \n",
            "  \n",
            "I will always be a child  \n",
            "And it was the same  \n",
            "The way that you wa\n",
            "\n",
            " \n",
            "I want to see the same  \n",
            "  \n",
            "I want to see the same  \n",
            "  \n",
            "I want to see the same  \n",
            "  \n",
            "I want to see ice  \n",
            "I want to see the way you want  \n",
            "I wanna be \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "g to run  \n",
            "  \n",
            "I can't get her your child  \n",
            "And the way they were so confusion  \n",
            "  \n",
            "And I don't know what I do?  \n",
            "  \n",
            "I wonder why, what you got me  \n",
            "I'\n",
            "\n",
            "h the street will she was the earth  \n",
            "  \n",
            "We got to be a bad song  \n",
            "  \n",
            "Oh we are the signs will send you  \n",
            "  \n",
            "We are wanting to see  \n",
            "We're gonna have \n",
            "\n",
            "m changin' like a closer  \n",
            "I can't get the one  \n",
            "  \n",
            "I'm gonna love her  \n",
            "I don't wanna drive the rain  \n",
            "  \n",
            "I can change the chance  \n",
            "We love you  \n",
            "  \n",
            "\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "et up now I'm a new rising morphin  \n",
            "Guess I went to smile with it  \n",
            "  \n",
            "There's nothing never rings  \n",
            "Our quentions New York  \n",
            "He so walled around  \n",
            "W\n",
            "\n",
            "  \n",
            "So many times I could have to give my feet  \n",
            "  \n",
            "You put your first oplipse  \n",
            "I'm talking to except things the whole, cill holes  \n",
            "  \n",
            "I tone broken,\n",
            "\n",
            "to be arust back  \n",
            "You got to meet your middle  \n",
            "The summer I knew you like to take me say it's time too  \n",
            "You could only be looking for  \n",
            "My lips and\n",
            "\n",
            "Epoch 9/10\n",
            "1886/1886 [==============================] - 442s 235ms/step - loss: 1.0296 - val_loss: 1.1511\n",
            "Epoch 10/10\n",
            "1886/1886 [==============================] - 443s 235ms/step - loss: 0.9960 - val_loss: 1.1426\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "s looking for  \n",
            "She was a heart  \n",
            "I was born to see the way  \n",
            "I was born to the same  \n",
            "  \n",
            "I can't believe in my heart  \n",
            "I want to tell you the same  \n",
            "\n",
            "\n",
            "me old stereo, storm  \n",
            "I want to be a chance  \n",
            "I was born to see the streets  \n",
            "And I want to see the same  \n",
            "  \n",
            "I want to see you to be  \n",
            "I want to see\n",
            "\n",
            " said \"I love you  \n",
            "  \n",
            "I wanna be on you, I will  \n",
            "I want to see you to see  \n",
            "I can't believe in your eyes  \n",
            "I want to see the same  \n",
            "I want to see th\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "I can change  \n",
            "But I know you'll be there  \n",
            "I just want to see  \n",
            "  \n",
            "I've seen my heart  \n",
            "  \n",
            "Well I got to stay at home  \n",
            "I will always see that you to\n",
            "\n",
            "s are playing the window  \n",
            "In a woman on the bad stone  \n",
            "On the street  \n",
            "The sin the past of control  \n",
            "When I said that it seems to say  \n",
            "You can live\n",
            "\n",
            "ith a child  \n",
            "We come from the moon like a rock in the ground  \n",
            "  \n",
            "It's time to speak  \n",
            "I got the one  \n",
            "  \n",
            "Don't want to do is stay and run  \n",
            "I need y\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "tle purer,  \n",
            "Tomorrow all your daddy and you could be  \n",
            "In my head of all creamin'  \n",
            "And they now will end like that seat goes  \n",
            "To the river's beer  \n",
            "\n",
            "e air you want is to dance  \n",
            "Than you feel like shown  \n",
            "But I'll never be the cost of you and I love you there,  \n",
            "Feelin' like a while  \n",
            "Putting on th\n",
            "\n",
            "ust  \n",
            "All like a cross the lies stin  \n",
            "[Into I Tith Perry by Dusty Shitcher-\n",
            "Praised on herself that sister, day beneath, \"We'll show him skappy  \n",
            "  \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1N8GRW9u0CEj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_songs1000_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs1000_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs1000_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jwR63ffiThLM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "50e84d3e-430e-459c-c3e9-081742f1ecd6"
      },
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "file = files[i]\n",
        "for file, n in zip(files[1:], nums[1:]):\n",
        "  print('Songs ', str(n), '==>', file)\n",
        "  model_name = 'char_'+'songs'+str(n)\n",
        "  textgen = textgenrnn(name=model_name)\n",
        "\n",
        "  train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "  # ~75 mins to train\n",
        "  train_function(\n",
        "      file_path=file,\n",
        "      new_model=True,\n",
        "      num_epochs=train_cfg['num_epochs'],\n",
        "      gen_epochs=train_cfg['gen_epochs'],\n",
        "      batch_size=train_cfg['batch_size'],\n",
        "      train_size=train_cfg['train_size'],\n",
        "      dropout=train_cfg['dropout'],\n",
        "      max_gen_length=train_cfg['max_gen_length'],\n",
        "      validation=train_cfg['validation'],\n",
        "      is_csv=train_cfg['is_csv'],\n",
        "      rnn_layers=model_cfg['rnn_layers'],\n",
        "      rnn_size=model_cfg['rnn_size'],\n",
        "      rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "      max_length=model_cfg['max_length'],\n",
        "      dim_embeddings=model_cfg['dim_embeddings'],\n",
        "      word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Songs  5000 ==> ./gdrive/My Drive/NLPProject/songs5000.txt\n",
            "Training new model w/ 4-layer, 128-cell Bidirectional LSTMs\n",
            "Training on 4,831,367 character sequences.\n",
            "Epoch 1/10\n",
            " 209/9436 [..............................] - ETA: 41:26 - loss: 3.4192"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UmDitnT0Ut4V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp char_songs5000_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs5000_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs5000_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp char_songs10000_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs10000_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs10000_weights.hdf5 ./gdrive/'My Drive'/NLPProject/\n",
        "\n",
        "! cp char_songs25000_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs25000_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp char_songs25000_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vN5JuIttSOzd"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-Level RNNs"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SRXbdrn4SOze",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "model_cfg = {\n",
        "    'rnn_size': 128,\n",
        "    'rnn_layers': 4,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 10,\n",
        "    'max_words': 10000,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': True,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': False,\n",
        "    'num_epochs': 50,\n",
        "    'gen_epochs': 5,\n",
        "    'batch_size': 512,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.25,\n",
        "    'max_gen_length': 80,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-B81V_sHorx8"
      },
      "cell_type": "markdown",
      "source": [
        "### Song Lyrics"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FDfTxnbKoryE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "file = files[i]\n",
        "\n",
        "model_name = 'char_'+'songs'+str(nums[i])\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7msUqnkVoryJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ~x mins to train\n",
        "train_function(\n",
        "    file_path=file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xTdiG1_woryR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp word_songs1000_config.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs1000_vocab.json ./gdrive/'My Drive'/NLPProject/\n",
        "! cp word_songs1000_weights.hdf5 ./gdrive/'My Drive'/NLPProject/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}